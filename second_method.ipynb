{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "30000\r"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import re\n",
    "# import spacy\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as T\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from emoji import UNICODE_EMOJI\n",
    "from util import *\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "file_path = 'Train_Tagged_Titles.tsv'\n",
    "df = pd.read_csv(file_path, sep=\"\\t\", dtype=str, keep_default_na=False, na_values=[\"\"], quoting=csv.QUOTE_NONE)\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "\n",
    "wv_from_bin = load_embedding_model()\n",
    "title_path = 'Listing_Titles.tsv'\n",
    "df_title = pd.read_csv(title_path, sep=\"\\t\", dtype=str, keep_default_na=False, na_values=[\"\"], quoting=csv.QUOTE_NONE)\n",
    "df_title.head()\n",
    "\n",
    "titles_x = []\n",
    "vocab = set()\n",
    "MAX_LEN = 0 \n",
    "for index, row in df_title.iterrows():\n",
    "    print(index,end='\\r')\n",
    "    sent_list = row['Title'].split(' ')\n",
    "    MAX_LEN = max(MAX_LEN, len(sent_list))\n",
    "    vocab.update(sent_list)\n",
    "    if index== 30000:\n",
    "        break\n",
    "df = df.fillna('NAN')\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for i in range(1,5001):\n",
    "    temp_df = df[df['Record Number']==str(i)]\n",
    "    tokens = []\n",
    "    tages = []\n",
    "    \n",
    "    for index, row in temp_df.iterrows():\n",
    "        if row['Tag'] == 'NAN':\n",
    "            tokens.append(row['Token'])\n",
    "            tages.append(temp)\n",
    "        else:\n",
    "            temp =  row['Tag']\n",
    "            tokens.append(row['Token'])\n",
    "            tages.append(row['Tag'])\n",
    "\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "    # doc = nlp(sentence)\n",
    "    # sentence = doc._.outcome_spellCheck\n",
    "    # tokens = sentence.split(' ')\n",
    "    x.append(tokens)\n",
    "    y.append(tages)\n",
    "    \n",
    "MAX_LEN = 0 \n",
    "for i in x:\n",
    "    MAX_LEN = max(len(i), MAX_LEN)\n",
    "MAX_LEN +=20\n",
    "\n",
    "target = set()\n",
    "for i in y:\n",
    "    target.update(i)\n",
    "    \n",
    "target_name_to_index = {}\n",
    "\n",
    "num = 1\n",
    "\n",
    "for i in target:\n",
    "    target_name_to_index[i]= num\n",
    "    target_name_to_index[num]= i\n",
    "    num+=1\n",
    "    \n",
    "target_name_to_index[0] = '[PAD]'\n",
    "target_name_to_index['[PAD]'] = 0\n",
    "\n",
    "CAT_NUM = int(len(target_name_to_index)/2)\n",
    "\n",
    "vector_space = []\n",
    "NUM_COMPONENTS= 290 #you can select any number of components.\n",
    "### Vocabulary cleaning\n",
    "\n",
    "\n",
    "\n",
    "for i in vocab:\n",
    "    w = vocab_clean(i)\n",
    "    vector_space.append(wv_from_bin[w])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Padding is a zero vector. \n",
    "wv_from_bin['[PAD]'] = np.zeros(300)+0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.00001, random_state=48)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    Pad_token  = 'NULL'\n",
    "    X_train[i]= [Pad_token,Pad_token] + X_train[i] + [Pad_token,Pad_token] \n",
    "for i in range(len(X_test)):\n",
    "    X_test[i]= [Pad_token,Pad_token] + X_test[i] + [Pad_token,Pad_token] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each input is a concatnation of three vectors. The vector of the word we are predicting the tag of, vector of the word before that word and vector of the word after that word. The vectors are weighted differently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55108, 1500)\n",
      "(13, 1500)\n",
      "(55108,)\n",
      "(13,)\n"
     ]
    }
   ],
   "source": [
    "NUM_COMPONENTS= 300\n",
    "x_train_vectorized = []\n",
    "\n",
    "\n",
    "for sentence_ind in range(len(X_train)):\n",
    "    for word_ind in range(2,len(X_train[sentence_ind])-2):\n",
    "            vector1 = wv_from_bin[vocab_clean(X_train[sentence_ind][word_ind-2])]*0.7\n",
    "            vector2 = wv_from_bin[vocab_clean(X_train[sentence_ind][word_ind-1])]*0.9\n",
    "            vector3 = wv_from_bin[vocab_clean(X_train[sentence_ind][word_ind])]\n",
    "            vector4 = wv_from_bin[vocab_clean(X_train[sentence_ind][word_ind+1])]*0.9\n",
    "            vector5 = wv_from_bin[vocab_clean(X_train[sentence_ind][word_ind+2])]*0.7\n",
    "            x_train_vectorized.append(np.array([vector1,vector2,vector3,vector4,vector5]).flatten())        \n",
    "        \n",
    "\n",
    "x_test_vectorized = []\n",
    "for sentence_ind in range(len(X_test)):\n",
    "    for word_ind in range(2,len(X_test[sentence_ind])-2):\n",
    "        vector1 = wv_from_bin[vocab_clean(X_test[sentence_ind][word_ind-2])]*0.7\n",
    "        vector2 = wv_from_bin[vocab_clean(X_test[sentence_ind][word_ind-1])]*0.9\n",
    "        vector3 = wv_from_bin[vocab_clean(X_test[sentence_ind][word_ind])]\n",
    "        vector4 = wv_from_bin[vocab_clean(X_test[sentence_ind][word_ind+1])]*0.6\n",
    "        vector5 = wv_from_bin[vocab_clean(X_test[sentence_ind][word_ind+2])]*0.5\n",
    "        x_test_vectorized.append(np.array([vector1,vector2,vector3,vector4,vector5]).flatten())        \n",
    "            \n",
    "x_test_vectorized = np.array(x_test_vectorized)\n",
    "x_train_vectorized = np.array(x_train_vectorized)\n",
    "\n",
    "print(x_train_vectorized.shape)\n",
    "print(x_test_vectorized.shape)\n",
    "\n",
    "\n",
    "\n",
    "y_digit_train = []\n",
    "for sentence_ind in range(len(y_train)):\n",
    "    for tag_ind in range(len(y_train[sentence_ind])):\n",
    "        y_digit_train.append(target_name_to_index[y_train[sentence_ind][tag_ind]])\n",
    "\n",
    "y_digit_train = np.array(y_digit_train)\n",
    "print(y_digit_train.shape)\n",
    "\n",
    "y_digit_test  = []\n",
    "for sentence_ind in range(len(y_test)):\n",
    "    for tag_ind in range(len(y_test[sentence_ind])):\n",
    "        y_digit_test.append(target_name_to_index[y_test[sentence_ind][tag_ind]])\n",
    "y_digit_test = np.array(y_digit_test)\n",
    "print(y_digit_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "train_dataset = []\n",
    "for i in range(len(x_train_vectorized)):\n",
    "    train_dataset.append((x_train_vectorized[i],\n",
    "                          y_digit_train[i]))\n",
    "    \n",
    "test_dataset = []\n",
    "for i in range(len(x_test_vectorized)):\n",
    "    test_dataset.append((x_test_vectorized[i],\n",
    "                          y_digit_test[i]))\n",
    "    \n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chang\\OneDrive\\Desktop\\test\\eBay_ML_Challenge_Dataset_2022\\second_method.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Desktop/test/eBay_ML_Challenge_Dataset_2022/second_method.ipynb#ch0000009?line=0'>1</a>\u001b[0m NUM_COMPONENT\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Desktop/test/eBay_ML_Challenge_Dataset_2022/second_method.ipynb#ch0000009?line=1'>2</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mLSTMTagger\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Desktop/test/eBay_ML_Challenge_Dataset_2022/second_method.ipynb#ch0000009?line=4'>5</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Desktop/test/eBay_ML_Challenge_Dataset_2022/second_method.ipynb#ch0000009?line=5'>6</a>\u001b[0m         \u001b[39m'''\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Desktop/test/eBay_ML_Challenge_Dataset_2022/second_method.ipynb#ch0000009?line=6'>7</a>\u001b[0m \u001b[39m        hidden_dim: can be anything, usually 32 or 64.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Desktop/test/eBay_ML_Challenge_Dataset_2022/second_method.ipynb#ch0000009?line=7'>8</a>\u001b[0m \u001b[39m        vocab_size: vocabulary size includes an index for padding.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Desktop/test/eBay_ML_Challenge_Dataset_2022/second_method.ipynb#ch0000009?line=8'>9</a>\u001b[0m \u001b[39m        output_size: We need to exclude the index for padding here.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chang/OneDrive/Desktop/test/eBay_ML_Challenge_Dataset_2022/second_method.ipynb#ch0000009?line=9'>10</a>\u001b[0m \u001b[39m        '''\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_COMPONENT=300\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, batch_size=64):\n",
    "        '''\n",
    "        hidden_dim: can be anything, usually 32 or 64.\n",
    "        vocab_size: vocabulary size includes an index for padding.\n",
    "        output_size: We need to exclude the index for padding here.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.l_relu =  nn.LeakyReLU(0.02)\n",
    "        self.fc1= nn.Linear(NUM_COMPONENTS*5, 2048)\n",
    "        self.bn05 = nn.BatchNorm1d(2048)\n",
    "        self.fc2 = nn.Linear(2048, 2048)\n",
    "        self.bn1 = nn.BatchNorm1d(2048)\n",
    "        self.fc3 = nn.Linear(2048, 2048)\n",
    "        self.bn2 = nn.BatchNorm1d(2048)\n",
    "        self.fc4 = nn.Linear(2048, 2048)\n",
    "        self.bn3 = nn.BatchNorm1d(2048)\n",
    "        self.fc5 = nn.Linear(2048, 2048)\n",
    "        self.bn4 = nn.BatchNorm1d(2048)\n",
    "        self.fc5 = nn.Linear(2048, 2048)\n",
    "        self.bn4 = nn.BatchNorm1d(2048)\n",
    "        \n",
    "        # self.fc6 = nn.Linear(2048, 2048)\n",
    "        # self.bn5 = nn.BatchNorm1d(2048)\n",
    "            \n",
    "        # self.fc7 = nn.Linear(2048, 2048)\n",
    "        # self.bn6 = nn.BatchNorm1d(2048) \n",
    "           \n",
    "        # self.fc8 = nn.Linear(2048, 1024)\n",
    "        # self.bn7 = nn.BatchNorm1d(1024)        \n",
    "        \n",
    "        self.fc9 = nn.Linear(2048, 33)\n",
    "        self.bn8 = nn.BatchNorm1d(33)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc10 = nn.Linear(33, CAT_NUM)\n",
    "        self.fc14 = nn.Linear(2048, 512)\n",
    "        \n",
    "        self.drop = nn.Dropout(0.05)\n",
    "        \n",
    "    def forward(self, words):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        sentences: padded sentences tensor. Each element of the tensor is an array of words.\n",
    "        X_lengths: length of sentence tensor. Each element of the tensor is the original\n",
    "                   length of the unpadded sentence.\n",
    "        Returns\n",
    "        -------\n",
    "        '''\n",
    "        # Dimensions of tensors:\n",
    "        # (Note that seq_len is max length)\n",
    "        # Shape of embedding (embeds): batch_size, seq_len, hidden_dim\n",
    "        # Shape of embedding post packing (embeds): batch_size, orig_len, hidden_dim\n",
    "        # Shape of self.hidden: (num_layers*num_directions, batch_size, hidden_dim)\n",
    "        # Shape of lstm_out: batch_size, seq_len, hidden_dim\n",
    "        # Shape of tag_scores: batch_size, 1\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        out14 = self.fc1(words)\n",
    "        out14 = self.bn05(out14)\n",
    "        out = self.l_relu(out14)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.l_relu(out)\n",
    "        # out = self.drop(out)\n",
    "            \n",
    "        out = self.fc3(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.l_relu(out)\n",
    "        # out = self.drop(out)\n",
    "                \n",
    "        out = self.fc4(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.l_relu(out)\n",
    "        # out = self.drop(out)\n",
    "        \n",
    "        out = self.fc5(out)\n",
    "        out = self.bn4(out)\n",
    "        out = self.l_relu(out)\n",
    "        # out = self.drop(out)        \n",
    "        # out = self.fc6(out)\n",
    "        # out = self.bn5(out)\n",
    "        # out = self.l_relu(out)\n",
    "\n",
    "        # out = self.fc7(out)\n",
    "        # out = self.bn6(out)\n",
    "        # out = self.l_relu(out)\n",
    "        \n",
    "        # out = self.fc8(out)\n",
    "        # out = self.bn7(out)\n",
    "        # out = self.l_relu(out)\n",
    "        \n",
    "        out = self.fc9(out)\n",
    "        out = self.bn8(out)\n",
    "        out = self.l_relu(out)\n",
    "        \n",
    "        out = self.fc10(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX LENTH: 42\n",
      "NUMBER OF PRINCEPLE COMPONENT: 300\n",
      "NUMBER OF CATAGORIES: 33\n"
     ]
    }
   ],
   "source": [
    "BATCHSIZE = 1024\n",
    "print(\"MAX LENTH:\",MAX_LEN)\n",
    "print(\"NUMBER OF PRINCEPLE COMPONENT:\",NUM_COMPONENTS)\n",
    "print(\"NUMBER OF CATAGORIES:\",CAT_NUM)\n",
    "device = torch.device(\"cuda\")\n",
    "network = LSTMTagger(batch_size = BATCHSIZE)\n",
    "model = network.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001,weight_decay=0.5E-10)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.93)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=BATCHSIZE)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=BATCHSIZE)\n",
    "loss_fn =  nn.CrossEntropyLoss( reduction='sum')\n",
    "def train(model, device, train_loader, optimizer, epoch, loss_fn,batch_size, log_interval=400):\n",
    "    model.train()\n",
    "    lose = 0 \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        batch_size = data.size()[0]\n",
    "        data, target = data.to(device).type(torch.cuda.FloatTensor), target.to(device).type(torch.cuda.LongTensor)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model.forward(data)\n",
    "        \n",
    "        loss =  F.nll_loss(y_pred,\n",
    "                        target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lose += loss.item()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "    return lose/len(train_loader.dataset)\n",
    "\n",
    "def test(model, device, test_loader, batch_size):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            batch_size = data.size()[0]\n",
    "            data, target = data.to(device).type(torch.cuda.FloatTensor), target.type(torch.cuda.LongTensor)\n",
    "            y_pred = model.forward(data)\n",
    "            test_loss +=  F.nll_loss(y_pred,\n",
    "                            target)\n",
    "            pred = y_pred.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return correct / len(test_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/55108 (0%)]\tLoss: 0.047022\n",
      "\n",
      "Test set: Average loss: -0.1065, Accuracy: 9/13 (69%)\n",
      "\n",
      "0.6923076923076923\n",
      "Train Epoch: 2 [0/55108 (0%)]\tLoss: -1.531150\n",
      "\n",
      "Test set: Average loss: -0.1223, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 3 [0/55108 (0%)]\tLoss: -1.720946\n",
      "\n",
      "Test set: Average loss: -0.1241, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 4 [0/55108 (0%)]\tLoss: -1.880757\n",
      "\n",
      "Test set: Average loss: -0.1282, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 5 [0/55108 (0%)]\tLoss: -2.029626\n",
      "\n",
      "Test set: Average loss: -0.1335, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 6 [0/55108 (0%)]\tLoss: -2.168331\n",
      "\n",
      "Test set: Average loss: -0.1405, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 7 [0/55108 (0%)]\tLoss: -2.297377\n",
      "\n",
      "Test set: Average loss: -0.1505, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 8 [0/55108 (0%)]\tLoss: -2.419702\n",
      "\n",
      "Test set: Average loss: -0.1589, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 9 [0/55108 (0%)]\tLoss: -2.548410\n",
      "\n",
      "Test set: Average loss: -0.1624, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 10 [0/55108 (0%)]\tLoss: -2.672610\n",
      "\n",
      "Test set: Average loss: -0.1692, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 11 [0/55108 (0%)]\tLoss: -2.811671\n",
      "\n",
      "Test set: Average loss: -0.1705, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 12 [0/55108 (0%)]\tLoss: -2.935823\n",
      "\n",
      "Test set: Average loss: -0.1785, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 13 [0/55108 (0%)]\tLoss: -3.072530\n",
      "\n",
      "Test set: Average loss: -0.1828, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 14 [0/55108 (0%)]\tLoss: -3.197250\n",
      "\n",
      "Test set: Average loss: -0.1882, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 15 [0/55108 (0%)]\tLoss: -3.329093\n",
      "\n",
      "Test set: Average loss: -0.1964, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 16 [0/55108 (0%)]\tLoss: -3.463094\n",
      "\n",
      "Test set: Average loss: -0.2052, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 17 [0/55108 (0%)]\tLoss: -3.595803\n",
      "\n",
      "Test set: Average loss: -0.2191, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 18 [0/55108 (0%)]\tLoss: -3.731336\n",
      "\n",
      "Test set: Average loss: -0.2317, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 19 [0/55108 (0%)]\tLoss: -3.867821\n",
      "\n",
      "Test set: Average loss: -0.2390, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 20 [0/55108 (0%)]\tLoss: -4.003763\n",
      "\n",
      "Test set: Average loss: -0.2532, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 21 [0/55108 (0%)]\tLoss: -4.150600\n",
      "\n",
      "Test set: Average loss: -0.2564, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 22 [0/55108 (0%)]\tLoss: -4.282322\n",
      "\n",
      "Test set: Average loss: -0.2661, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 23 [0/55108 (0%)]\tLoss: -4.421265\n",
      "\n",
      "Test set: Average loss: -0.2772, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 24 [0/55108 (0%)]\tLoss: -4.559453\n",
      "\n",
      "Test set: Average loss: -0.2870, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 25 [0/55108 (0%)]\tLoss: -4.699097\n",
      "\n",
      "Test set: Average loss: -0.2933, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 26 [0/55108 (0%)]\tLoss: -4.840016\n",
      "\n",
      "Test set: Average loss: -0.3041, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 27 [0/55108 (0%)]\tLoss: -4.981662\n",
      "\n",
      "Test set: Average loss: -0.3159, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 28 [0/55108 (0%)]\tLoss: -5.121228\n",
      "\n",
      "Test set: Average loss: -0.3294, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 29 [0/55108 (0%)]\tLoss: -5.266865\n",
      "\n",
      "Test set: Average loss: -0.3342, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 30 [0/55108 (0%)]\tLoss: -5.398779\n",
      "\n",
      "Test set: Average loss: -0.3358, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 31 [0/55108 (0%)]\tLoss: -5.550860\n",
      "\n",
      "Test set: Average loss: -0.3371, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 32 [0/55108 (0%)]\tLoss: -5.690536\n",
      "\n",
      "Test set: Average loss: -0.3532, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 33 [0/55108 (0%)]\tLoss: -5.829044\n",
      "\n",
      "Test set: Average loss: -0.3612, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 34 [0/55108 (0%)]\tLoss: -5.972033\n",
      "\n",
      "Test set: Average loss: -0.3805, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 35 [0/55108 (0%)]\tLoss: -6.113274\n",
      "\n",
      "Test set: Average loss: -0.3796, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 36 [0/55108 (0%)]\tLoss: -6.255313\n",
      "\n",
      "Test set: Average loss: -0.3907, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 37 [0/55108 (0%)]\tLoss: -6.398308\n",
      "\n",
      "Test set: Average loss: -0.3940, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 38 [0/55108 (0%)]\tLoss: -6.542971\n",
      "\n",
      "Test set: Average loss: -0.3960, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 39 [0/55108 (0%)]\tLoss: -6.687806\n",
      "\n",
      "Test set: Average loss: -0.4139, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 40 [0/55108 (0%)]\tLoss: -6.828348\n",
      "\n",
      "Test set: Average loss: -0.4062, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 41 [0/55108 (0%)]\tLoss: -6.975653\n",
      "\n",
      "Test set: Average loss: -0.4417, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 42 [0/55108 (0%)]\tLoss: -7.120023\n",
      "\n",
      "Test set: Average loss: -0.4234, Accuracy: 10/13 (77%)\n",
      "\n",
      "0.7692307692307693\n",
      "Train Epoch: 43 [0/55108 (0%)]\tLoss: -7.245048\n",
      "\n",
      "Test set: Average loss: -0.4861, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 44 [0/55108 (0%)]\tLoss: -7.379988\n",
      "\n",
      "Test set: Average loss: -0.5103, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 45 [0/55108 (0%)]\tLoss: -7.531675\n",
      "\n",
      "Test set: Average loss: -0.5004, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 46 [0/55108 (0%)]\tLoss: -7.681647\n",
      "\n",
      "Test set: Average loss: -0.4997, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 47 [0/55108 (0%)]\tLoss: -7.833251\n",
      "\n",
      "Test set: Average loss: -0.5042, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 48 [0/55108 (0%)]\tLoss: -7.981182\n",
      "\n",
      "Test set: Average loss: -0.5256, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 49 [0/55108 (0%)]\tLoss: -8.128178\n",
      "\n",
      "Test set: Average loss: -0.5266, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 50 [0/55108 (0%)]\tLoss: -8.274487\n",
      "\n",
      "Test set: Average loss: -0.5458, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 51 [0/55108 (0%)]\tLoss: -8.425859\n",
      "\n",
      "Test set: Average loss: -0.5615, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 52 [0/55108 (0%)]\tLoss: -8.565014\n",
      "\n",
      "Test set: Average loss: -0.5697, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 53 [0/55108 (0%)]\tLoss: -8.703102\n",
      "\n",
      "Test set: Average loss: -0.5803, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 54 [0/55108 (0%)]\tLoss: -8.842047\n",
      "\n",
      "Test set: Average loss: -0.5885, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 55 [0/55108 (0%)]\tLoss: -8.986104\n",
      "\n",
      "Test set: Average loss: -0.5976, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 56 [0/55108 (0%)]\tLoss: -9.127458\n",
      "\n",
      "Test set: Average loss: -0.5986, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 57 [0/55108 (0%)]\tLoss: -9.269765\n",
      "\n",
      "Test set: Average loss: -0.6121, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 58 [0/55108 (0%)]\tLoss: -9.413761\n",
      "\n",
      "Test set: Average loss: -0.6223, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 59 [0/55108 (0%)]\tLoss: -9.557004\n",
      "\n",
      "Test set: Average loss: -0.6322, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 60 [0/55108 (0%)]\tLoss: -9.699844\n",
      "\n",
      "Test set: Average loss: -0.6423, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 61 [0/55108 (0%)]\tLoss: -9.847276\n",
      "\n",
      "Test set: Average loss: -0.6431, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 62 [0/55108 (0%)]\tLoss: -9.981460\n",
      "\n",
      "Test set: Average loss: -0.6566, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 63 [0/55108 (0%)]\tLoss: -10.120718\n",
      "\n",
      "Test set: Average loss: -0.6602, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 64 [0/55108 (0%)]\tLoss: -10.261094\n",
      "\n",
      "Test set: Average loss: -0.6710, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 65 [0/55108 (0%)]\tLoss: -10.399975\n",
      "\n",
      "Test set: Average loss: -0.6789, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 66 [0/55108 (0%)]\tLoss: -10.541071\n",
      "\n",
      "Test set: Average loss: -0.6910, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 67 [0/55108 (0%)]\tLoss: -10.682361\n",
      "\n",
      "Test set: Average loss: -0.6988, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 68 [0/55108 (0%)]\tLoss: -10.823587\n",
      "\n",
      "Test set: Average loss: -0.7087, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 69 [0/55108 (0%)]\tLoss: -10.965495\n",
      "\n",
      "Test set: Average loss: -0.7161, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 70 [0/55108 (0%)]\tLoss: -11.107602\n",
      "\n",
      "Test set: Average loss: -0.7293, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 71 [0/55108 (0%)]\tLoss: -11.250981\n",
      "\n",
      "Test set: Average loss: -0.7379, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 72 [0/55108 (0%)]\tLoss: -11.384094\n",
      "\n",
      "Test set: Average loss: -0.7412, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 73 [0/55108 (0%)]\tLoss: -11.517760\n",
      "\n",
      "Test set: Average loss: -0.7524, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 74 [0/55108 (0%)]\tLoss: -11.652832\n",
      "\n",
      "Test set: Average loss: -0.7619, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 75 [0/55108 (0%)]\tLoss: -11.789862\n",
      "\n",
      "Test set: Average loss: -0.7672, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 76 [0/55108 (0%)]\tLoss: -11.926579\n",
      "\n",
      "Test set: Average loss: -0.7751, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 77 [0/55108 (0%)]\tLoss: -12.062719\n",
      "\n",
      "Test set: Average loss: -0.7837, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 78 [0/55108 (0%)]\tLoss: -12.200164\n",
      "\n",
      "Test set: Average loss: -0.7953, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 79 [0/55108 (0%)]\tLoss: -12.338082\n",
      "\n",
      "Test set: Average loss: -0.8069, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 80 [0/55108 (0%)]\tLoss: -12.476048\n",
      "\n",
      "Test set: Average loss: -0.8158, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 81 [0/55108 (0%)]\tLoss: -12.614841\n",
      "\n",
      "Test set: Average loss: -0.8199, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 82 [0/55108 (0%)]\tLoss: -12.742766\n",
      "\n",
      "Test set: Average loss: -0.8238, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 83 [0/55108 (0%)]\tLoss: -12.871829\n",
      "\n",
      "Test set: Average loss: -0.8342, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 84 [0/55108 (0%)]\tLoss: -13.002009\n",
      "\n",
      "Test set: Average loss: -0.8443, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 85 [0/55108 (0%)]\tLoss: -13.133997\n",
      "\n",
      "Test set: Average loss: -0.8542, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 86 [0/55108 (0%)]\tLoss: -13.265743\n",
      "\n",
      "Test set: Average loss: -0.8522, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 87 [0/55108 (0%)]\tLoss: -13.396690\n",
      "\n",
      "Test set: Average loss: -0.8734, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 88 [0/55108 (0%)]\tLoss: -13.528948\n",
      "\n",
      "Test set: Average loss: -0.8397, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 89 [0/55108 (0%)]\tLoss: -13.635144\n",
      "\n",
      "Test set: Average loss: -0.8906, Accuracy: 12/13 (92%)\n",
      "\n",
      "0.9230769230769231\n",
      "Train Epoch: 90 [0/55108 (0%)]\tLoss: -13.691867\n",
      "\n",
      "Test set: Average loss: -0.9169, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 91 [0/55108 (0%)]\tLoss: -13.903385\n",
      "\n",
      "Test set: Average loss: -0.8945, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 92 [0/55108 (0%)]\tLoss: -14.075377\n",
      "\n",
      "Test set: Average loss: -0.9254, Accuracy: 11/13 (85%)\n",
      "\n",
      "0.8461538461538461\n",
      "Train Epoch: 93 [0/55108 (0%)]\tLoss: -14.211388\n",
      "\n",
      "Test set: Average loss: -0.9381, Accuracy: 12/13 (92%)\n",
      "\n",
      "0.9230769230769231\n",
      "Train Epoch: 94 [0/55108 (0%)]\tLoss: -14.342996\n",
      "\n",
      "Test set: Average loss: -0.9450, Accuracy: 12/13 (92%)\n",
      "\n",
      "0.9230769230769231\n",
      "Train Epoch: 95 [0/55108 (0%)]\tLoss: -14.471848\n",
      "\n",
      "Test set: Average loss: -0.9545, Accuracy: 12/13 (92%)\n",
      "\n",
      "0.9230769230769231\n",
      "Train Epoch: 96 [0/55108 (0%)]\tLoss: -14.601770\n",
      "\n",
      "Test set: Average loss: -0.9625, Accuracy: 12/13 (92%)\n",
      "\n",
      "0.9230769230769231\n",
      "Train Epoch: 97 [0/55108 (0%)]\tLoss: -14.728477\n",
      "\n",
      "Test set: Average loss: -0.9714, Accuracy: 12/13 (92%)\n",
      "\n",
      "0.9230769230769231\n",
      "Train Epoch: 98 [0/55108 (0%)]\tLoss: -14.858793\n",
      "\n",
      "Test set: Average loss: -0.9783, Accuracy: 12/13 (92%)\n",
      "\n",
      "0.9230769230769231\n",
      "Train Epoch: 99 [0/55108 (0%)]\tLoss: -14.988693\n",
      "\n",
      "Test set: Average loss: -0.9871, Accuracy: 12/13 (92%)\n",
      "\n",
      "0.9230769230769231\n",
      "Train Epoch: 100 [0/55108 (0%)]\tLoss: -15.117982\n",
      "\n",
      "Test set: Average loss: -0.9940, Accuracy: 12/13 (92%)\n",
      "\n",
      "0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "train_e=[]\n",
    "test_e = []\n",
    "for i in range(100):\n",
    "      \n",
    "    train_e.append(train(model, device, train_loader,\n",
    "          optimizer, i,loss_fn=loss_fn,\n",
    "          batch_size=BATCHSIZE))\n",
    "    accuracy_ = test(model, device, test_loader, batch_size=BATCHSIZE)\n",
    "    print(accuracy_)\n",
    "#     if accuracy_ > 0.99:\n",
    "#           break \n",
    "    test_e.append(accuracy_)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\r"
     ]
    }
   ],
   "source": [
    "def predict_tag(sample):\n",
    "    sample = [vocab_clean(word) for word in sample]\n",
    "    sentence = []\n",
    "    sample = [Pad_token,Pad_token] + sample + [Pad_token,Pad_token] \n",
    "    for word_ind in range(2,len(sample)-2):\n",
    "        vector1 = wv_from_bin[vocab_clean(sample[word_ind-2])]*0.7\n",
    "        vector2 = wv_from_bin[vocab_clean(sample[word_ind-1])]*0.9\n",
    "        vector3 = wv_from_bin[vocab_clean(sample[word_ind])]\n",
    "        vector4 = wv_from_bin[vocab_clean(sample[word_ind+1])]*0.6\n",
    "        vector5 = wv_from_bin[vocab_clean(sample[word_ind+2])]*0.5\n",
    "        sentence.append(np.array([vector1,vector2,vector3,vector4,vector5]).flatten()) \n",
    "    sentence = np.array(sentence)\n",
    "\n",
    "    out_list = []\n",
    "    for word_ind in range(sentence.shape[0]):\n",
    "        word = sentence[word_ind,:]\n",
    "        sample = torch.tensor(word).to(device).type(torch.cuda.FloatTensor)\n",
    "        model.eval()\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        y_pred = model.forward(sample.view(1,5*300))\n",
    "        y_pred = y_pred.cpu()\n",
    "        y_pred = y_pred.detach().numpy()\n",
    "        y_pred = y_pred.squeeze()\n",
    "\n",
    "        y_pred = y_pred.argmax(axis = 0)\n",
    "\n",
    "        out_list.append(target_name_to_index[y_pred])\n",
    "\n",
    "    return out_list\n",
    "\n",
    "def tokenizer(title):\n",
    "    tokens = title.split(' ')\n",
    "    out = []\n",
    "    for token in tokens:\n",
    "        out.append(token)\n",
    "    return out\n",
    "\n",
    "submission = {'Record number':[], \"Aspect Name\":[], \"Aspect Value\":[]}\n",
    "for num in range(5001,30001):\n",
    "    print(int(((num-5000)/(30000-5000))*100), end = '\\r')\n",
    "    tokens = tokenizer(df_title['Title'][num-1])\n",
    "\n",
    "    out_list=predict_tag(tokens)\n",
    "    temp=None\n",
    "    jump = False\n",
    "    for i in range(min(len(out_list),len(tokens))):\n",
    "        if jump:\n",
    "            jump = False \n",
    "            continue\n",
    "        if tokens[i].lower() == 'hand' and i != len(tokens)-1 and   tokens[i+1].lower() == 'bag':\n",
    "            jump = True\n",
    "            submission['Record number'].append(num)\n",
    "            submission['Aspect Name'].append('Type')\n",
    "            submission['Aspect Value'].append(tokens[i] + ' ' + tokens[i+1])\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        if tokens[i].lower() == 'cc' and i != len(tokens)-1 and   tokens[i+1].lower() == 'logo':\n",
    "            jump = True\n",
    "            submission['Record number'].append(num)\n",
    "            submission['Aspect Name'].append('Accents')\n",
    "            submission['Aspect Value'].append(tokens[i] + ' ' + tokens[i+1])\n",
    "            continue\n",
    "        if tokens[i].lower() == 'leather' and i != 0 and   out_list[i-1] == 'Material':\n",
    "            submission['Aspect Value'][-1] = tokens[i-1] + ' ' + tokens[i]\n",
    "            continue\n",
    "        if out_list[i] == temp and (out_list[i] == 'Model' or\n",
    "                                    out_list[i] == 'Brand' or \n",
    "                                    out_list[i] == 'Theme' or\n",
    "                                    out_list[i] == 'Trim Material' or\n",
    "                                    out_list[i] == 'Strap Drop' or \n",
    "                                    out_list[i] == 'Character' or\n",
    "                                    out_list[i] == 'Character Family' or\n",
    "                                    out_list[i] == 'Features' or\n",
    "                                    out_list[i] == 'Handle Drop' or\n",
    "                                    out_list[i] == 'Shoulder Strap' or\n",
    "                                    out_list[i] == 'Hardware Material' or\n",
    "                                    out_list[i] =='Handle Style' or\n",
    "                                    out_list[i] =='Measurement, dimension' or\n",
    "                                    out_list[i] == 'Pocket Type' or\n",
    "                                    out_list[i] == 'Product Line'):\n",
    "            prev_name = submission['Aspect Value'][-1]\n",
    "            submission['Record number'].append(num)\n",
    "            submission['Aspect Name'].append(out_list[i])\n",
    "            submission['Aspect Value'].append(prev_name + ' ' + tokens[i])\n",
    "        else:\n",
    "            submission['Record number'].append(num)\n",
    "            submission['Aspect Name'].append(out_list[i])\n",
    "            submission['Aspect Value'].append(tokens[i])\n",
    "            temp = out_list[i]\n",
    "submission= pd.DataFrame(submission)\n",
    "submission.to_csv('submission.csv', sep ='\\t', index = False,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEEN Montclair Mini Brown Khaki Crosshatched Pattern Travel Crossbody Bag Purse\n",
      "['KEEN', 'Montclair', 'Mini', 'Brown', 'Khaki', 'Crosshatched', 'Pattern', 'Travel', 'Crossbody', 'Bag', 'Purse']\n",
      "['Size', 'Color', 'Color', 'Pattern', 'No Tag', 'Occasion', 'Type']\n"
     ]
    }
   ],
   "source": [
    "num = 280024\n",
    "print(df_title['Title'][num])\n",
    "tokens = tokenizer(df_title['Title'][num])\n",
    "\n",
    "print(tokens)\n",
    "out_list=predict_tag(tokens)\n",
    "print(out_list)\n",
    "# print(y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e48aae3af0582e7bec53cd0b470c702fdde06057170277f48501fa0fdc9baa8a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ebay')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
